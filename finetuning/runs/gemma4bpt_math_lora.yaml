name: gemma4bpt_math_5ep_lora_v2
method: grid
parameters:
  peak_lr:
    values: [3.2e-7, 1.0e-6, 3.2e-6, 1.0e-5, 3.2e-5, 1.0e-4, 3.2e-4, 1.0e-3]
  seed:
    values: [0, 1, 2, 3]
program: finetune.py
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - --model_variant='gemma3-4b'
  - --param_dtype='bfloat16'
  - --lora_rank=16
  - --optimizer_name='adam'
  - --batch_size=1
  - --n_epochs=5
  - --b2=0.997
  - --eval_batch_size=128
  - --wandb_mode='online'
  - ${args}
