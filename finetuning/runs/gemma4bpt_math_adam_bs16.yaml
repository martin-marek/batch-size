name: gemma4bpt_math_5ep_adam_bs16_v2
method: grid
parameters:
  peak_lr:
    values: [3.2e-8, 1.0e-7, 3.2e-7, 1.0e-6, 3.2e-6, 1.0e-5, 3.2e-5, 1.0e-4]
  seed:
    values: [0, 1, 2, 3]
program: finetune.py
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - --model_variant='gemma3-4b'
  - --param_dtype='float32'
  - --optimizer_name='adam'
  - --batch_size=16
  - --n_epochs=5
  - --b2=0.95
  - --eval_batch_size=128
  - --wandb_mode='online'
  - ${args}
