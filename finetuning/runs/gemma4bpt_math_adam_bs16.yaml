name: gemma4bpt_math_5ep_adam_bs16_v1
method: grid
parameters:
  peak_lr:
    values: [3.2e-8, 1.0e-7, 3.2e-7, 1.0e-6, 3.2e-6, 1.0e-5, 3.2e-5, 1.0e-4]
  seed:
    values: [0, 1, 2, 3, 4, 5, 6, 7]
program: finetune.py
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - --model_variant='gemma3-4b'
  - --optimizer_name='adam'
  - --batch_size=16
  - --n_epochs=5
  - --b2=0.95
  - --n_eval_samples=512
  - --eval_batch_size=128
  - --logging=True
  - --log_every_steps=3
  - ${args}
