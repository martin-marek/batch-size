{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1749056727116,
     "user": {
      "displayName": "Martin Marek",
      "userId": "04932572550491068578"
     },
     "user_tz": -60
    },
    "id": "laglInP5rzv8",
    "outputId": "5dbc6cd4-0244-442e-a52e-246ea2bd14f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 29 11:22:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA B200                    On  |   00000000:DC:00.0 Off |                    0 |\n",
      "| N/A   22C    P0            143W / 1000W |       0MiB / 183359MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5727,
     "status": "ok",
     "timestamp": 1749056737071,
     "user": {
      "displayName": "Martin Marek",
      "userId": "04932572550491068578"
     },
     "user_tz": -60
    },
    "id": "euko7CXVK-Jy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749056737087,
     "user": {
      "displayName": "Martin Marek",
      "userId": "04932572550491068578"
     },
     "user_tz": -60
    },
    "id": "cYbgxeyHKhgs"
   },
   "outputs": [],
   "source": [
    "# helpers\n",
    "def sizeof_fmt(num):\n",
    "    for unit in (\"\", \"K\", \"M\", \"G\", \"T\"):\n",
    "        if abs(num) < 1000:\n",
    "            return f\"{num:.2f}{unit}B\"\n",
    "        num /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"Muon that batches over >2D layers, based on https://github.com/KellerJordan/Muon/blob/master/muon.py\"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(p)\n",
    "                update = muon_update(p.grad, state[\"momentum_buffer\"], beta=group[\"momentum\"])\n",
    "                p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                # p.add_(update.reshape(p.shape), alpha=-group[\"lr\"]) # <-- CHANGE: No longer need reshape as update preserves shape\n",
    "                p.add_(update, alpha=-group[\"lr\"]) # <-- CHANGED\n",
    "\n",
    "        return loss\n",
    "\n",
    "def muon_update(grad, momentum, beta=0.95, ns_steps=5, nesterov=True):\n",
    "    momentum.lerp_(grad, 1 - beta)\n",
    "    update = grad.lerp_(momentum, beta) if nesterov else momentum\n",
    "    # if update.ndim == 4: # for the case of conv filters <-- CHANGE: Removed this block that flattens the tensor.\n",
    "    #     update = update.view(len(update), -1)\n",
    "    update = zeropower_via_newtonschulz5(update, steps=ns_steps)\n",
    "    # update *= max(1, grad.size(-2) / grad.size(-1))**0.5 <-- CHANGE: Swapped numerator/denominator to match JAX logic.\n",
    "    update *= max(1, grad.size(-1) / grad.size(-2))**0.5 # <-- CHANGED\n",
    "    return update\n",
    "\n",
    "def zeropower_via_newtonschulz5(G, steps: int):\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "    \n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1749056737124,
     "user": {
      "displayName": "Martin Marek",
      "userId": "04932572550491068578"
     },
     "user_tz": -60
    },
    "id": "E3LhnfGIdYP9"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, L, D, V, H=128, dtype=None):\n",
    "        super().__init__()\n",
    "        self.token_embed_in = nn.Embedding(V, D, dtype=dtype)\n",
    "        self.token_embed_out = nn.Linear(D, V, bias=False, dtype=dtype)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(D, H, dtype) for _ in range(L)])\n",
    "        self.out_ln = nn.RMSNorm(D, elementwise_affine=False, dtype=dtype)\n",
    "\n",
    "    def forward(self, x, y=None): # [B, S]\n",
    "\n",
    "        # token embedding\n",
    "        h = self.token_embed_in(x) # [B, T, D]\n",
    "\n",
    "        # transformer blocks\n",
    "        for block in self.blocks:\n",
    "            h = checkpoint(block, h, use_reentrant=False)\n",
    "\n",
    "        # project back to vocabulary\n",
    "        h = self.out_ln(h)\n",
    "        logits = self.token_embed_out(h) # [B, T, V]\n",
    "\n",
    "        # get loss\n",
    "        loss = F.cross_entropy(logits.flatten(end_dim=-2), y.flatten())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, D, H, dtype):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.RMSNorm(D, elementwise_affine=False, dtype=dtype)\n",
    "        self.ln2 = nn.RMSNorm(D, elementwise_affine=False, dtype=dtype)\n",
    "        self.attn = MultiHeadAttention(D, H, dtype)\n",
    "        self.mlp = Mlp(D, dtype)\n",
    "\n",
    "    def forward(self, x): # [B, T, D]\n",
    "        x = x + self.attn(self.ln1(x)) # attention block\n",
    "        return x + self.mlp(self.ln2(x)) # MLP block\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Causal attention layer.\"\"\"\n",
    "    def __init__(self, D, H, dtype):\n",
    "        super().__init__()\n",
    "        N = D // H # number of heads\n",
    "        self.qkv_proj = Einsum('BTd,SNdH->SBTNH', (3, N, D, H), dtype=dtype)\n",
    "        self.out_proj = Einsum('BTnh,nhD->BTD', (N, H, D), dtype=dtype)\n",
    "        self.query_norm = nn.RMSNorm(H, elementwise_affine=False, dtype=dtype)\n",
    "        self.key_norm = nn.RMSNorm(H, elementwise_affine=False, dtype=dtype)\n",
    "\n",
    "    def forward(self, x): # [B, T, D]\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # input projection\n",
    "        q, k, v = self.qkv_proj(x) # [B, T, N, H]\n",
    "\n",
    "        # qk-norm\n",
    "        q = self.query_norm(q)\n",
    "        k = self.key_norm(k)\n",
    "\n",
    "        # position embedding\n",
    "        # (ommited)\n",
    "\n",
    "        # attention\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True) # [B, T, N, H]\n",
    "\n",
    "        # output projection followed by contraction back to original dims\n",
    "        out = self.out_proj(out) # [B, T, D]\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\"\"\"\n",
    "    def __init__(self, D, dtype):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=D, out_features=4*D, bias=False, dtype=dtype)\n",
    "        self.fc2 = nn.Linear(in_features=4*D, out_features=D, bias=False, dtype=dtype)\n",
    "\n",
    "    def forward(self, x): # [B, T, D]\n",
    "        h = F.gelu(self.fc1(x)) # [B, T, F]\n",
    "        return self.fc2(h) # [B, T, D]\n",
    "\n",
    "\n",
    "class Einsum(nn.Module):\n",
    "    def __init__(self, einsum_str, kernel_shape, dtype=None):\n",
    "        super().__init__()\n",
    "        self.einsum_str = einsum_str\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_shape, dtype=dtype))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.einsum(self.einsum_str, x, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4142,
     "status": "ok",
     "timestamp": 1749056760614,
     "user": {
      "displayName": "Martin Marek",
      "userId": "04932572550491068578"
     },
     "user_tz": -60
    },
    "id": "RTdHEnqYQUTm",
    "outputId": "c32b4d5f-afb5-4ae0-d9fd-a4ff153d2285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_params=13_181_601_960\n",
      "size of model: 26.36GB\n",
      "size of opt. state: 0.00B\n",
      "max. memory allocated: 27.58GB\n",
      "size of \"other\": 1.21GB\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "\n",
    "    # model\n",
    "    V = 50257\n",
    "    with torch.device('cuda'):\n",
    "        # model = TransformerDecoder(L=12, D=768, V=V, dtype=torch.bfloat16) # 124M\n",
    "        # model = TransformerDecoder(L=24, D=2048, V=V, dtype=torch.bfloat16) # 1.3B\n",
    "        model = TransformerDecoder(L=40, D=5140, V=V, dtype=torch.bfloat16) # 13B\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{n_params=:_}')\n",
    "    print('size of model:', sizeof_fmt(2*n_params))\n",
    "\n",
    "    # standard optimizer (not fused)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), foreach=False)\n",
    "    # optimizer_dict = {'opt': optimizer}\n",
    "\n",
    "    # fused optimizer\n",
    "    # based on https://lightning.ai/pages/community/tutorial/faster-pytorch-training-by-reducing-peak-memory/\n",
    "    optimizer_dict = {p:torch.optim.SGD([p], foreach=False) for p in model.parameters()} # all params\n",
    "    # optimizer_dict = {p:Muon([p]) for p in model.blocks.parameters()} # non-embedding params\n",
    "    # optimizer_dict |= {p:torch.optim.Adam([p], foreach=False) for p in [*model.token_embed_in.parameters(), *model.token_embed_out.parameters()]} # embedding params\n",
    "    def optimizer_hook(parameter):\n",
    "        optimizer_dict[parameter].step()\n",
    "        optimizer_dict[parameter].zero_grad()\n",
    "    for p in model.parameters():\n",
    "        p.register_post_accumulate_grad_hook(optimizer_hook)\n",
    "\n",
    "    # define training step\n",
    "    def step():\n",
    "        T = 1024\n",
    "        x = torch.randint(V, [1, T], dtype=torch.int32, device='cuda')\n",
    "        y = torch.randint(V, [1, T], dtype=torch.int64, device='cuda')\n",
    "        loss = model(x, y)\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "    # warm up model\n",
    "    for _ in range(2):\n",
    "        step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # get optimzier state size\n",
    "    opt_num_params = 0\n",
    "    for p, opt in optimizer_dict.items():\n",
    "        opt_state = opt.state_dict()['state']\n",
    "        for s1 in opt_state.values():\n",
    "            for x in s1.values():\n",
    "                opt_num_params += x.numel()\n",
    "    print('size of opt. state:', sizeof_fmt(2*opt_num_params))\n",
    "\n",
    "    # plot step trace\n",
    "    # with torch.profiler.profile(record_shapes=True, profile_memory=True, with_stack=True) as p:\n",
    "    #     step()\n",
    "    # p.export_memory_timeline('stack.html', 'cuda:0')\n",
    "\n",
    "    # print max. memory during step\n",
    "    torch.cuda.reset_peak_memory_stats(\"cuda:0\")\n",
    "    step()\n",
    "    max_mem = torch.cuda.max_memory_allocated(\"cuda:0\")\n",
    "    print('max. memory allocated:', sizeof_fmt(max_mem))\n",
    "\n",
    "    # compute size of 'other'\n",
    "    other_size = max_mem - 2*n_params - 2*opt_num_params\n",
    "    print('size of \"other\":', sizeof_fmt(other_size))\n",
    "\n",
    "    # manully free memory (required given the circular reference btw model and optimizer)\n",
    "    del model; optimizer_dict.clear()\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2818,
     "status": "aborted",
     "timestamp": 1749056489279,
     "user": {
      "displayName": "Martin Marek",
      "userId": "04932572550491068578"
     },
     "user_tz": -60
    },
    "id": "ahvL8r6hKnNN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKei/Qc69MaUinTDbhqtr8",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
